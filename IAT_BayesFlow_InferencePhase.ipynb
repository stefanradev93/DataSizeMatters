{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import binom\n",
    "from functools import partial\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Import our bayesflow lib\n",
    "from deep_bayes.models import BayesFlow, InvariantNetwork\n",
    "from deep_bayes.training import train_online\n",
    "from deep_bayes.losses import maximum_likelihood_loss\n",
    "from deep_bayes.viz import plot_true_est_scatter, plot_true_est_posterior\n",
    "import deep_bayes.diagnostics as diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "if tf.__version__.startswith('1'):\n",
    "    tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def diffusion_trial(v, a, zr, ndt_c, ndt_w, dt, max_steps):\n",
    "    \"\"\"Simulates a trial from the diffusion model.\"\"\"\n",
    "\n",
    "    n_steps = 0.\n",
    "    x = a * zr\n",
    "\n",
    "    # Simulate a single DM path\n",
    "    while (x > 0 and x < a and n_steps < max_steps):\n",
    "\n",
    "        # DDM equation\n",
    "        x += v*dt + np.sqrt(dt) * np.random.normal()\n",
    "\n",
    "        # Increment step\n",
    "        n_steps += 1.0\n",
    "\n",
    "    rt = n_steps * dt\n",
    "    return rt + ndt_c if x > 0. else -rt - ndt_w\n",
    "\n",
    "@njit\n",
    "def simulate_diffusion_condition(n_trials, v, a, zr, ndt_c, ndt_w, dt=0.005, max_steps=10000):\n",
    "    \"\"\"Simulates a diffusion process over an entire condition.\"\"\"\n",
    "    \n",
    "    x = np.empty(n_trials)\n",
    "    for i in range(n_trials):\n",
    "        x[i] = diffusion_trial(v, a, zr, ndt_c, ndt_w, dt, max_steps)\n",
    "    return x\n",
    "\n",
    "\n",
    "@njit\n",
    "def simulate_diffusion_2_conds(theta, n_trials, dt=0.005, max_steps=10000):\n",
    "    \"\"\"Simulates a diffusion process for 2 conditions with 7 parameters (v1, v2, a1, a2, ndt1, ndt2, zr=0.5)\"\"\"\n",
    "    \n",
    "    n_trials_c1 = n_trials[0]\n",
    "    n_trials_c2 = n_trials[1]\n",
    "    \n",
    "    \n",
    "    v1, v2, a1, a2, ndt_c, ndt_w = theta\n",
    "    rt_c1 = simulate_diffusion_condition(n_trials_c1, v1, a1, 0.5, ndt_c, ndt_w, dt, max_steps)\n",
    "    rt_c2 = simulate_diffusion_condition(n_trials_c2, v2, a2, 0.5, ndt_c, ndt_w, dt, max_steps)\n",
    "    rts = np.concatenate((rt_c1, rt_c2))\n",
    "    return rts\n",
    "\n",
    "def data_generator(batch_size, n_obs=None, to_tensor=True, n_obs_min=25, n_obs_max=120):\n",
    "    \"\"\"\n",
    "    Runs the forward model 'batch_size' times by first sampling fromt the prior\n",
    "    theta ~ p(theta) and running x ~ p(x|theta) with the specified n_obs. If \n",
    "    n_obs is None, random number of trials for each condition are generated.\n",
    "    ----------\n",
    "    \n",
    "    Arguments:\n",
    "    batch_size : int -- the number of samples to draw from the prior\n",
    "    n_obs      : tuple (int, int) or None -- the numebr of observations to draw from p(x|theta)\n",
    "                                  for each condition\n",
    "    n_obs_min  : int -- the minimum number of observations per condition\n",
    "    n_obs_max  : int -- the maximum number of observations per condition\n",
    "    to_tensor  : boolean -- converts theta and x to tensors if True\n",
    "    ----------\n",
    "    \n",
    "    Output:\n",
    "    theta : tf.Tensor or np.ndarray of shape (batch_size, theta_dim) - the data gen parameters \n",
    "    x     : tf.Tensor of np.ndarray of shape (batch_size, n_obs, x_dim)  - the generated data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample from prior\n",
    "    # theta is a np.array of shape (batch_size, theta_dim)\n",
    "    theta = prior(batch_size)\n",
    "    \n",
    "    if n_obs is None:\n",
    "        n_obs = np.random.randint(n_obs_min, n_obs_max+1, 2)\n",
    "    \n",
    "    # Generate data\n",
    "    # x is a np.ndarray of shape (batch_size x n_obs, x_dim)\n",
    "    x = np.apply_along_axis(simulate_diffusion_2_conds, axis=1, arr=theta, n_trials=n_obs)\n",
    "    \n",
    "    # Assign conditions\n",
    "    cond_arr = np.stack( batch_size * [np.concatenate((np.zeros(n_obs[0]), np.ones(n_obs[1])))] )\n",
    "    x = np.stack((x, cond_arr), axis=-1)\n",
    "    \n",
    "    # Convert to tensor, if specified \n",
    "    if to_tensor:\n",
    "        theta = tf.convert_to_tensor(theta, dtype=tf.float32)\n",
    "        x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    return {'theta': theta, 'x': x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Network Structure\n",
    "Here, we will define the basic outline of a permutation-invariant neural network which maps raw reaction times data to outcomes.\n",
    "<br>\n",
    "See https://arxiv.org/pdf/1901.06082.pdf (p.28) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     53,
     97
    ]
   },
   "outputs": [],
   "source": [
    "class InvariantModule(tf.keras.Model):\n",
    "    \"\"\"Implements an invariant nn module as proposed by Bloem-Reddy and Teh (2019).\"\"\"\n",
    "\n",
    "    def __init__(self, meta, pooler=tf.reduce_mean):\n",
    "        \"\"\"\n",
    "        Creates an invariant function with mean pooling.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        meta : dict -- a dictionary with hyperparameter name - values\n",
    "        \"\"\"\n",
    "\n",
    "        super(InvariantModule, self).__init__()\n",
    "\n",
    "\n",
    "        self.module = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(**meta['dense_inv_args'])\n",
    "            for _ in range(meta['n_dense_inv'])\n",
    "        ])\n",
    "        \n",
    "        self.pooler = pooler\n",
    "            \n",
    "\n",
    "        self.post_pooling_dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(**meta['dense_inv_args'])\n",
    "            for _ in range(meta['n_dense_inv'])\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Transofrms the input into an invariant representation.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n, m) - the input where n is the 'time' or 'samples' dimensions\n",
    "            over which pooling is performed and m is the input dimensionality\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        out : tf.Tensor of shape (batch_size, h_dim) -- the pooled and invariant representation of the input\n",
    "        \"\"\"\n",
    "\n",
    "        # Embed\n",
    "        x_emb = self.module(x)\n",
    "\n",
    "        # Pool representation\n",
    "        pooled = self.pooler(x_emb, axis=1)\n",
    "    \n",
    "        # Increase representational power\n",
    "        out = self.post_pooling_dense(pooled)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EquivariantModule(tf.keras.Model):\n",
    "    \"\"\"Implements an equivariant nn module as proposed by Bloem-Reddy and Teh (2019).\"\"\"\n",
    "\n",
    "    def __init__(self, meta):\n",
    "        \"\"\"\n",
    "        Creates an equivariant neural network consisting of a FC network with\n",
    "        equal number of hidden units in each layer and an invariant module\n",
    "        with the same FC structure.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        meta : dict -- a dictionary with hyperparameter name - values\n",
    "        \"\"\"\n",
    "\n",
    "        super(EquivariantModule, self).__init__()\n",
    "\n",
    "        self.module = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(**meta['dense_equiv_args'])\n",
    "            for _ in range(meta['n_dense_equiv'])\n",
    "        ])\n",
    "\n",
    "        self.invariant_module = InvariantModule(meta)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Transofrms the input into an equivariant representation.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n, m) - the input where n is the 'time' or 'samples' dimensions\n",
    "            over which pooling is performed and m is the input dimensionality\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        out : tf.Tensor of shape (batch_size, h_dim) -- the pooled and invariant representation of the input\n",
    "        \"\"\"\n",
    "\n",
    "        x_inv = self.invariant_module(x)\n",
    "        x_inv = tf.stack([x_inv] * int(x.shape[1]), axis=1) # Repeat x_inv n times\n",
    "        x = tf.concat((x_inv, x), axis=-1)\n",
    "        out = self.module(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class InvariantNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Implements a network which parameterizes a\n",
    "    permutationally invariant function according to Bloem-Reddy and Teh (2019).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, meta):\n",
    "        \"\"\"\n",
    "        Creates a permutationally invariant network\n",
    "        consisting of two equivariant modules and one invariant module.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        meta : dict -- hyperparameter settings for the equivariant and invariant modules\n",
    "        \"\"\"\n",
    "\n",
    "        super(InvariantNetwork, self).__init__()\n",
    "\n",
    "        self.equiv = tf.keras.Sequential([\n",
    "            EquivariantModule(meta)\n",
    "            for _ in range(meta['n_equiv'])\n",
    "        ])\n",
    "        self.inv = InvariantModule(meta)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Transofrms the input into a permutationally invariant\n",
    "        representation by first passing it through multiple equivariant\n",
    "        modules in order to increase representational power.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n, m) - the input where n is the\n",
    "        'samples' dimensions over which pooling is performed and m is the input dimensionality\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        out : tf.Tensor of shape (batch_size, h_dim) -- the pooled and invariant representation of the input\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.equiv(x)\n",
    "        out = self.inv(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter settings and model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network structure\n",
    "summary_meta = {\n",
    "    'dense_inv_args'   :  dict(units=64, activation='elu', kernel_initializer='glorot_normal'),\n",
    "    'dense_equiv_args' :  dict(units=64, activation='elu', kernel_initializer='glorot_normal'),\n",
    "    'dense_post_args'  :  dict(units=64, activation='elu', kernel_initializer='glorot_normal'),\n",
    "    'n_equiv'          :  2,\n",
    "    'n_dense_inv'      :  3,\n",
    "    'n_dense_equiv'    :  3,\n",
    "}\n",
    "\n",
    "# Network hyperparameters\n",
    "inv_meta = {\n",
    "    'n_units': [128, 128, 128],\n",
    "    'activation': 'elu',\n",
    "    'w_decay': 0.00000,\n",
    "    'initializer': 'glorot_uniform'\n",
    "}\n",
    "n_inv_blocks = 4\n",
    "\n",
    "# Forward model hyperparameters\n",
    "param_names = [r'$v_1$', r'$v_2$', r'$a_1$', r'$a_2$', r'$\\tau_{c}$', r'$\\tau_{w}$']\n",
    "theta_dim = len(param_names)\n",
    "n_test = 300\n",
    "n_obs_max = 60\n",
    "n_obs_min = 60\n",
    "n_obs_test = (60, 60)\n",
    "\n",
    "# Utility for online learning\n",
    "data_gen = partial(data_generator, n_obs_min=n_obs_min, n_obs_max=n_obs_max)\n",
    "\n",
    "\n",
    "# Training and optimizer hyperparameters\n",
    "ckpt_file = \"iat_bayesflow\"\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "iterations_per_epoch = 1000\n",
    "n_samples_posterior = 2000\n",
    "clip_value = 5.\n",
    "\n",
    "learning_rate = 0.001\n",
    "if tf.__version__.startswith('1'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_net = InvariantNetwork(summary_meta)\n",
    "model = BayesFlow(inv_meta, n_inv_blocks, theta_dim, summary_net=summary_net, permute=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint manager\n",
    "Used for saving/loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from C:/Users/User1/Downloads/DataSizeMatters-main/DataSizeMatters-main/iat_bayesflow\\ckpt-final\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, 'C:/Users/User1/Downloads/DataSizeMatters-main/DataSizeMatters-main/iat_bayesflow', max_to_keep=5)\n",
    "checkpoint.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on all real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iat_rt_file(file_to_read, black_not_african):\n",
    "    \"\"\"\n",
    "    Reads and preprocesses a single IAT file.\n",
    "    \"\"\"\n",
    "    \n",
    "    if black_not_african:\n",
    "        pairs_to_retain = ['Black people/Bad,White people/Good', \n",
    "                            'White people/Bad,Black people/Good', \n",
    "                            'White people/Good,Black people/Bad', \n",
    "                            'Black people/Good,White people/Bad']\n",
    "    else:\n",
    "        pairs_to_retain =['African Americans/Bad,European Americans/Good', \n",
    "                            'European Americans/Bad,African Americans/Good', \n",
    "                            'European Americans/Good,African Americans/Bad', \n",
    "                            'African Americans/Good,European Americans/Bad']\n",
    "\n",
    "\n",
    "    # We only need certain columsn from the data, so we can indicate pandas to only read\n",
    "    # a subset of them and save memory. The problem is, that the data has been inappropriately\n",
    "    # saved and when the header is read, there are whitespaces in the data, so we shall assume\n",
    "    # that all relevant columns have the same order in all datafiles\n",
    "    cols_to_use_str = ['block_number', 'block_pairing_definition', 'trial_latency', 'trial_error', 'session_id']\n",
    "    cols_to_use_idx = [0, 3, 10, 11, 12]\n",
    "    valid_values_error = [0, 1, '1', '0'] # Use for some datasets containing invalid values\n",
    "    \n",
    "    df = pd.read_csv(PATH + file_to_read, delimiter='\\t', usecols=cols_to_use_idx)\n",
    "\n",
    "    # Fix absolutely inadequate data handling in project implicit\n",
    "    # There are whitespaces in the columns, to we need to get rid of them\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    assert all([c1 == c2 for c1, c2 in zip(df.columns, cols_to_use_str)]), 'Expected column order does not match!'\n",
    " \n",
    "\n",
    "    # Retain only relevant pairs\n",
    "    df = df[df['block_pairing_definition'].isin(pairs_to_retain)]\n",
    "    \n",
    "    # Remove invalid ids\n",
    "    df = df[df['trial_error'].isin(valid_values_error)]\n",
    "    \n",
    "    # Ensure correct types\n",
    "    df = df.astype({\"block_number\": int, \n",
    "                    \"block_pairing_definition\": str,\n",
    "                    \"trial_latency\": int,\n",
    "                    \"trial_error\": int})\n",
    "\n",
    "    # Remove sessions with 0 latency\n",
    "    zero_latency_id = df[df['trial_latency']==0]['session_id'].to_list()\n",
    "    df = df[~df['session_id'].isin(zero_latency_id)]\n",
    "\n",
    "    # Remove sessions with 0 errors\n",
    "    zero_errors_id = df.groupby('session_id').sum()\n",
    "    zero_errors_id = zero_errors_id[zero_errors_id['trial_error'] == 0].index.to_list()\n",
    "    df = df[~df['session_id'].isin(zero_errors_id)]\n",
    "\n",
    "    # Convert RTs in ms\n",
    "    df.loc[:, 'trial_latency'] = df['trial_latency'] / 1000\n",
    "\n",
    "    # Code compatible vs incompatible\n",
    "    # changes for different types of stimuli! (black/white vs african american/european american)\n",
    "    if black_not_african:\n",
    "        df['trial_compatible'] = np.where((df['block_pairing_definition'] == \"Black people/Bad,White people/Good\") | \n",
    "                                      (df['block_pairing_definition'] == 'White people/Good,Black people/Bad'), 1, 0)\n",
    "    else:\n",
    "        df['trial_compatible'] = np.where((df['block_pairing_definition'] == \"African Americans/Bad,European Americans/Good\") | \n",
    "                                      (df['block_pairing_definition'] == 'European Americans/Good,African Americans/Bad'), 1, 0)\n",
    "\n",
    "    \n",
    "    # Sort dataframe according to session id\n",
    "    df = df.sort_values('session_id')\n",
    "    \n",
    "    # Convert rt dataset to dictionary (session_id - np.array of rt/errors/comopatible)\n",
    "    df_rt_dict = df.groupby('session_id')['trial_latency'].apply(np.array).to_dict()\n",
    "    df_err_dict = df.groupby('session_id')['trial_error'].apply(np.array).to_dict()\n",
    "    df_comp_dict = df.groupby('session_id')['trial_compatible'].apply(np.array).to_dict()\n",
    "    df_block_dict = df.groupby('session_id')['block_number'].apply(np.array).to_dict()\n",
    "    \n",
    "    rts_data = {}\n",
    "    for k1, k2, k3, k4 in zip(df_rt_dict, df_err_dict, df_comp_dict, df_block_dict):\n",
    "        assert k1 == k2 == k3 == k4, 'Key mismatch within the same dataset, something went wrong!'\n",
    "        rts_data[k1] = np.c_[df_rt_dict[k1], df_err_dict[k1], df_comp_dict[k1], df_block_dict[k1]]\n",
    "    return rts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset_name, black_not_african):\n",
    "    \"\"\"\n",
    "    Preprocesses a single IAT file and combines with outcomes.\n",
    "    \"\"\"    \n",
    "    # Prepare a list for current chunk data/outcomes\n",
    "    # Since we don't know how many will survive the next step\n",
    "    # we initialize lists which we late convert to np.arrays\n",
    "\n",
    "    X_arr = []\n",
    "    y_arr = []\n",
    "    \n",
    "    # Read in file\n",
    "    data_dict = read_iat_rt_file(dataset_name, black_not_african)\n",
    "    print('Successfully read datafile', dataset_name, '.')\n",
    "    print('Converting to X and y arrays...')\n",
    "    \n",
    "    # Loop through sessions and get those RT arrays for which we have outcomes\n",
    "    for key in data_dict.keys():\n",
    "        outcome = outcomes_dict.get(key)\n",
    "        \n",
    "        # Add only if outcome for the dataset is present\n",
    "        if outcome is not None:\n",
    "            \n",
    "            # Add only if 120 trials and 2 outcomes present\n",
    "            if data_dict[key].shape == (120, 4) and outcomes_dict[key].shape == (2,):\n",
    "                X_arr.append(data_dict[key])\n",
    "                y_arr.append(outcomes_dict[key])\n",
    "            \n",
    "    # X becomes a 3D array (N_datasets x 120 x 4)\n",
    "    # y becomes a 2D array (N_datasets x 2)\n",
    "    X_arr = np.stack(X_arr)\n",
    "    y_arr = np.stack(y_arr)\n",
    "\n",
    "    print('Final shape of RTs chunk: ', X_arr.shape)\n",
    "    print('Final shape of outcomes chunk: ', y_arr.shape)\n",
    "    \n",
    "    return X_arr, y_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning_300 (X_test, y_test):\n",
    "    \"\"\"\n",
    "    Get data in correct format and remove cases with more than 12 trials < 300 ms\n",
    "    \"\"\"        \n",
    "    # Get X_test into the correct format:\n",
    "    rts = np.where(X_test[:, :, 1], -X_test[:, :, 0], X_test[:, :, 0])\n",
    "    comps = X_test[:, :, 2]\n",
    "    X_test = np.stack((rts, comps), axis=2)\n",
    "\n",
    "    # Exclusion criterion (< 0.3)\n",
    "    idx_300 = (np.abs(X_test[:, :, 0]) < 0.3).sum(axis=1) <= 12\n",
    "    X_test = X_test[idx_300, :, :]\n",
    "\n",
    "    # Keep only corresponding y\n",
    "    y_test = y_test[idx_300, :]\n",
    "    y_test = np.int64(y_test)\n",
    "    \n",
    "    # Set trials with latency <300 ms or >10 seconds to 0\n",
    "    X_test[:,:,0][np.abs(X_test[:,:,0]) < .3] = 0\n",
    "    X_test[:,:,0][np.abs(X_test[:,:,0]) > 10] = 0\n",
    "    X_test = np.float32(X_test)\n",
    "\n",
    "    return X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summaries(samples_dm):\n",
    "    \"\"\"\n",
    "    Compute summary statistics and correlations\n",
    "    \"\"\"          \n",
    "    param_means = samples_dm.mean(axis=0)\n",
    "    param_medians = np.median(samples_dm,axis=0)\n",
    "    param_stds = np.std(samples_dm,axis=0)\n",
    "    param_q025 = np.quantile(samples_dm, .025, axis=0)  \n",
    "    param_q975 = np.quantile(samples_dm, .975, axis=0)  \n",
    "    \n",
    "    corr = tfp.stats.correlation(samples_dm, sample_axis=0, event_axis=2).numpy()\n",
    "    corr_array = np.concatenate((\n",
    "        corr[:,0,1:6],corr[:,1,2:6], corr[:,2,3:6], corr[:,3,4:6], corr[:,4,5:6]), axis=1)\n",
    "    \n",
    "    estimates = np.concatenate((param_means, param_medians, param_stds,\n",
    "                                param_q025, param_q975, corr_array), axis=1) \n",
    "    \n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(X_test, estimates, param_means, y_test):\n",
    "    \"\"\"\n",
    "    Delete datasets with out-of-prior parameter estimates (from data and estimates)\n",
    "    \"\"\"\n",
    "    good_cases = ((param_means[:,0]>.1) & (param_means[:,0]<7.) & #v1 between .1 and 7\n",
    "                  (param_means[:,1]>.1) & (param_means[:,1]<7.) & #v2 between .1 and 7\n",
    "                  (param_means[:,2]>.1) & (param_means[:,2]<4.) & #a1 between .1 and 4\n",
    "                  (param_means[:,3]>.1) & (param_means[:,3]<4.) & #v2 between .1 and 4\n",
    "                  (param_means[:,4]>.1) & (param_means[:,4]<3.) & #tplus between .1 and 3\n",
    "                  (param_means[:,5]>.1) & (param_means[:,5]<7.))  #tminus between .1 and 7\n",
    "\n",
    "    \n",
    "    estimates_clean = estimates[good_cases,:]\n",
    "    data_chunk_clean = X_test[good_cases,:,:]\n",
    "    y_clean = y_test[good_cases,:]\n",
    "    \n",
    "    return data_chunk_clean, estimates_clean, y_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings\n",
    "# Path to data\n",
    "PATH = 'D:/iat/data/'\n",
    "# Path to outcomes\n",
    "PATH_OUTCOME = 'D:/iat/outcomes/'\n",
    "\n",
    "# where to save files\n",
    "PATH_TO_SAVE = 'D:/iat/pickle/'\n",
    "\n",
    "# Choose type of stimulus to process (Black/African American)\n",
    "black_not_african = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in outcomes, drop missing, sort\n",
    "outcomes = pd.read_csv(PATH_OUTCOME + 'outcomes_with_id_161718.csv', header=0, na_values='NA')\n",
    "outcomes = outcomes.dropna()\n",
    "outcomes = outcomes.sort_values('session_id')\n",
    "\n",
    "# Convert outcomes to a dictionary for instant lookup; use only cases containing age and session id\n",
    "outcomes_id2_dict = outcomes.groupby('session_id')['id2'].apply(int).to_dict()\n",
    "outcomes_age_dict = outcomes.groupby('session_id')['age'].apply(int).to_dict()\n",
    "\n",
    "\n",
    "outcomes_dict = {}\n",
    "for k1, k2 in zip(outcomes_id2_dict, outcomes_age_dict):\n",
    "    assert k1 == k2 \n",
    "    outcomes_dict[k1] = np.array([outcomes_id2_dict[k1], \n",
    "                                  outcomes_age_dict[k2]])\n",
    "    \n",
    "# Note, that outcomes dict will be a global dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read datafile 2018iat10.txt .\n",
      "Converting to X and y arrays...\n",
      "Final shape of RTs chunk:  (20553, 120, 4)\n",
      "Final shape of outcomes chunk:  (20553, 2)\n",
      "WARNING:tensorflow:From C:\\Users\\User1\\Downloads\\DataSizeMatters-main\\DataSizeMatters-main\\deep_bayes\\models.py:277: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorflow_probability\\python\\stats\\sample_stats.py:459: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\User1\\miniconda3\\lib\\site-packages\\tensorflow_probability\\python\\stats\\sample_stats.py:693: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n",
      "2018iat10.txt done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read datafile 2018iat11.txt .\n",
      "Converting to X and y arrays...\n",
      "Final shape of RTs chunk:  (20863, 120, 4)\n",
      "Final shape of outcomes chunk:  (20863, 2)\n",
      "2018iat11.txt done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read datafile 2018iat12.txt .\n",
      "Converting to X and y arrays...\n",
      "Final shape of RTs chunk:  (20313, 120, 4)\n",
      "Final shape of outcomes chunk:  (20313, 2)\n",
      "2018iat12.txt done\n",
      "Successfully read datafile 2018iat13.txt .\n",
      "Converting to X and y arrays...\n",
      "Final shape of RTs chunk:  (6171, 120, 4)\n",
      "Final shape of outcomes chunk:  (6171, 2)\n",
      "2018iat13.txt done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read datafile 2018iat2.txt .\n",
      "Converting to X and y arrays...\n",
      "Final shape of RTs chunk:  (20688, 120, 4)\n",
      "Final shape of outcomes chunk:  (20688, 2)\n",
      "2018iat2.txt done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read datafile 2018iat3.txt .\n",
      "Converting to X and y arrays...\n",
      "Final shape of RTs chunk:  (20063, 120, 4)\n",
      "Final shape of outcomes chunk:  (20063, 2)\n",
      "2018iat3.txt done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read datafile 2018iat4.txt .\n",
      "Converting to X and y arrays...\n",
      "Final shape of RTs chunk:  (20128, 120, 4)\n",
      "Final shape of outcomes chunk:  (20128, 2)\n",
      "2018iat4.txt done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read datafile 2018iat5.txt .\n",
      "Converting to X and y arrays...\n",
      "Final shape of RTs chunk:  (20085, 120, 4)\n",
      "Final shape of outcomes chunk:  (20085, 2)\n",
      "2018iat5.txt done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read datafile 2018iat6.txt .\n",
      "Converting to X and y arrays...\n",
      "Final shape of RTs chunk:  (19729, 120, 4)\n",
      "Final shape of outcomes chunk:  (19729, 2)\n",
      "2018iat6.txt done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read datafile 2018iat7.txt .\n",
      "Converting to X and y arrays...\n",
      "Final shape of RTs chunk:  (20168, 120, 4)\n",
      "Final shape of outcomes chunk:  (20168, 2)\n",
      "2018iat7.txt done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read datafile 2018iat8.txt .\n",
      "Converting to X and y arrays...\n",
      "Final shape of RTs chunk:  (20558, 120, 4)\n",
      "Final shape of outcomes chunk:  (20558, 2)\n",
      "2018iat8.txt done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\miniconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read datafile 2018iat9.txt .\n",
      "Converting to X and y arrays...\n",
      "Final shape of RTs chunk:  (20347, 120, 4)\n",
      "Final shape of outcomes chunk:  (20347, 2)\n",
      "2018iat9.txt done\n"
     ]
    }
   ],
   "source": [
    "#This is where the magic happens\n",
    "\n",
    "# 1. Store all data-set chunk names in a list\n",
    "datasets = os.listdir(PATH)\n",
    "\n",
    "# 2. For each chunk\n",
    "for dataset_name in datasets:\n",
    "    \n",
    "    # 2.1 Load chunk\n",
    "    X_test, y_test = prepare_data(dataset_name, black_not_african)\n",
    " \n",
    "    # 2.2 Preprocess chunk \n",
    "    # 2.2.1 Apply IAT exclusion criteria (conservative)\n",
    "    # 2.2.2 Format data for NN, negative coding\n",
    "    # 2.2.3 Add 0s for <0.3, and >10\n",
    "    X_test, y_test = data_cleaning_300(X_test, y_test)\n",
    "      \n",
    "    # 2.3 Estimate chunk\n",
    "    samples_dm = np.concatenate([model.sample(x, n_samples=2000,\n",
    "                                              to_numpy=True) for x in np.array_split(X_test, 20)], axis=1)\n",
    "                \n",
    "    # 2.4 Compute summaries: means, medians, stds, Q0.025, Q0.0975, post_corr\n",
    "    estimates = compute_summaries(samples_dm)\n",
    "    param_means = estimates[:,0:6]\n",
    "\n",
    "    # 2.5 Post-processing \n",
    "        # 2.5.1 find inices of implausible (out of prior) parameter means\n",
    "        # 2.5.2 remove from estimates\n",
    "        # 2.5.3 remove from datasets\n",
    "    data_chunk_clean, estimates_clean, y_clean = post_processing(X_test, estimates, param_means, y_test)\n",
    "      \n",
    "    # 2.6 Store everything together (serialized, pickle.dump) as a dict with keys \n",
    "    dict_to_store = {'data_array': data_chunk_clean, 'est_array': estimates_clean, \"outcome_array\": y_clean}\n",
    "    pickle.dump(dict_to_store,\n",
    "                open(PATH_TO_SAVE +str(black_not_african)+ str(dataset_name.replace('.txt', '.p') ),\"wb\"))\n",
    "    print(str(dataset_name)+ \" done\")\n",
    "\n",
    "# 3. Celebrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "175.733px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
